{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re \n",
    "import spacy\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def extract_name(doc: spacy.tokens.Doc) -> str:\n",
    "    \"\"\"Extract the person's name using NER (Named Entity Recognition).\"\"\"\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "    # Fallback: assume name is at the top of the resume (first line)\n",
    "    first_line = doc.text.split('\\n')[0].strip()\n",
    "    return first_line if first_line else \"Name not found\"\n",
    "\n",
    "def extract_email(text: str) -> str:\n",
    "    \"\"\"Extract email address using regex.\"\"\"\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    match = re.search(email_pattern, text)\n",
    "    return match.group(0) if match else \"Email not found\"\n",
    "\n",
    "def extract_phone(text: str) -> str:\n",
    "    \"\"\"Extract phone number using regex.\"\"\"\n",
    "    phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
    "    match = re.search(phone_pattern, text)\n",
    "    return match.group(0) if match else \"Phone not found\"\n",
    "\n",
    "def extract_skills(doc, skills_list):\n",
    "    \"\"\"Extract skills by matching against a predefined list.\"\"\"\n",
    "    found_skills = set()\n",
    "    text = doc.text.lower()\n",
    "    for skill in skills_list:\n",
    "        if skill.lower() in text:\n",
    "            found_skills.add(skill)\n",
    "    return list(found_skills) if found_skills else [\"No skills matched\"]\n",
    "\n",
    "def extract_education(doc):\n",
    "    \"\"\"Extract education details.\"\"\"\n",
    "    education_keywords = {\"university\", \"college\", \"institute\", \"school\", \"bachelor\", \"master\", \"phd\", \"degree\"}\n",
    "    education = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"ORG\", \"DATE\"} and any(keyword in ent.text.lower() for keyword in education_keywords):\n",
    "            education.append(ent.text)\n",
    "    return education if education else [\"Education not found\"]\n",
    "\n",
    "def parse_resume(model, pdf_path, skills_list):\n",
    "    \"\"\"Main function to parse resume and extract information.\"\"\"\n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text:\n",
    "        return {\"error\": \"No text extracted from resume\"}\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = model(text)\n",
    "\n",
    "    # Extract information\n",
    "    resume_data = {\n",
    "        \"name\": extract_name(doc),\n",
    "        \"email\": extract_email(text),\n",
    "        \"phone\": extract_phone(text),\n",
    "        \"skills\": extract_skills(doc, skills_list),\n",
    "        \"education\": extract_education(doc)\n",
    "    }\n",
    "    return resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harry Nguyen\\nÂ§ hainguyen2903.github.io/gitprofile | (cid:239) linkedin.com/in/nguyenphuchai | # hainguyen29031412@gmail.com\\nEDUCATION\\nUniversity of Technology Sydney (UTS) August 2023 - August 2025\\nMasterâ€™s Degree in Data Science (Postgraduate Excellence International Scholarship)\\nUniversity of Engineering and Technology, Vietnam National University August 2018 - August 2022\\nBachelorâ€™s Degree in Computer Science\\nGPA: 3.51/4.0 Thesis score: 9.1/10\\nSKILLS AND KNOWLEDGE\\nBackground Knowledge Machine Learning, Computer Vision, Natural Language Processing\\nData Analysis Skills Tableau, Seaborn, Matplotlib, AWS\\nProgram Languages Python, SQL\\nData Enigineer Skills Azure, Databrick, Airflow, dbt\\nSoft Skills Teamwork, Self-Studying, Leadership, Problem Solving\\nWORKING EXPERIENCE\\nMachine Learning Engineer (SiliconCube Company) July 2022 - June 2023\\nâ€¢ Smart Parking System - Car Detection and License Plate Recognition\\nAn automatic parking monitoring system, which includes license plate recognition and parking slots monitoring.\\nResponsibility:\\nâ€“ Experimented, error analysed and intergrated multiple models (YOLOv5, WPOD-Net, ...), resulted in more\\nthan 98% in term of accuracy.\\nâ€“ Deployed trained model on NVIDIA Jetson Xavier device. Built server-client protocol to verified system pipeline\\nand dockerized product.\\nâ€¢ Smart Advisor for Online Learning Platform\\nAn AI system for tracking student knowledge level and recommending essential learning materials based on their historical\\nacademic performance.\\nResponsibility:\\nâ€“ Experimented, error analysed and intergrated multiple deep models to achieve high performance.\\nâ€“ Created pipeline for system continuous training and monitoring using Wandb.\\nComputer Vision Research Engineer (AI Lab - UET) April 2020 - August 2022\\nâ€¢ Real Time Multiple Cameras Surveillance System [Link]\\nAreal-timeMultipleTargetsTrackingsystemwith50camerasand12differentobservationareasina5-floorbuilding.\\nResponsibility:\\nâ€“ Researched, developed and ensembled several detection models for comparative analysis (mostly YOLOv5 and\\nCenterNet).\\nâ€“ Research, optimized and benchmarked different tracking models (DeepSORT, FairMOT), resulted in approxi-\\nmately 85% in term of MOTA metric on collected dataset.\\nâ€“ Leaded data team and annotation pipeline, including data collection, extraction, annotation and quality\\nverification.\\nâ€“ Created partial automation pipeline with re-identification models to support annotation process, resulted\\nin x2 annotation speed compared to traditional method.\\nâ€“ Collaborated to generate the largest multiple-camera tracking dataset compared to existing datasets in\\nterms of camera, tracking area, and human ID quantities.\\nâ€“ Collaborated to design new algorithm for tracking across multiple cameras using different techniques, namely\\ncamera calibration, top-view representation, feature matching, etc.DATA SCIENCE PROJECTS\\nâ€¢ Airbnb ad-hoc analyses with Big Data November 2024\\nEnd-to-end ELT pipeline and ad hoc analysis from raw data to key insights for the optimization of Airbnb strategies.\\nResponsibility:\\nâ€“ Deployed a production-ready ELT data pipeline, automating tasks with Apache Airflow and dbt Cloud.\\nâ€“ Organized data using a medallion architecture and enabled ad-hoc analysis with data marts.\\nâ€“ Delivered ad-hoc analyses that provided actionable insights for key business decisions.\\nâ€¢ NSW Traffic Analysis for Road Maintenance Optimization [Link] June 2024\\nA story-telling project with advanced visualizations for NSW Road Maintenance Optimization.\\nResponsibility:\\nâ€“ Performed different data processing and transformation techniques on the the source data set (dedupicating,\\naggregation, joining, etc.).\\nâ€“ LeveragedTableauforcomplexvisualizations,includinggeographicalheatmaps,multi-levelbarcharts,time-series\\ncharts, and combined charts.\\nâ€“ Designed storyline with effective visualizations (i.e., color highlighting, group comparisons, etc.) to deliver valu-\\nable business insights to audience.\\nâ€¢ AWS Elastic Beanstalk PaaS for LAMP stack application June 2024\\nThe project aims to implement a scalable, elastic, highly available, and fault-tolerant PaaS for business needs.\\nResponsibility:\\nâ€“ Implemented a PaaS with different AWS services, including EC2, Load Balancer, Auto Scaling Group, RDS,\\nVPC, etc. that allows businesses to fully customize for their LAMP stack application.\\nâ€¢ Banking Customer Segmentation [Link] June 2024\\nThe project leverages the bankâ€™s extensive transactional data collected over the past three years to develop machine\\nlearning for segmenting customers based on spending behaviors, facilitating personalized marketing.\\nResponsibility:\\nâ€“ Performed comprehensive data exploratory to identify important features for model development.\\nâ€“ Leveragedoriginalfeaturestogenerateamorereliablefeatureset(i.e. RFMvalues)forbettercustomerclustering.\\nâ€“ ExperimentedvariousclusteringmodelswhichincludesDBSCANandKMeans,combiningwithPCAtooptimize\\nmodel weight and performance.\\nâ€¢ Telecom Customer Understanding [Link] August 2023\\nThe project aims to obtain a comprehensive understanding of customer groups from companyâ€™s past marketing cam-\\npaigns to identify new strategies for the upcoming campaign through the development of the EDA and ML model\\ndevelopment.\\nResonsibility:\\nâ€“ Applied various EDA techniques (visualization, univariate and bivariate analysis, etc.) to draw comprehensive\\ninsights of customers and external contexts of the given dataset.\\nâ€“ Leveraged hypothesis testing to verify generated hypothesis of variable relationships.\\nâ€“ Developed various classification models, including XGBoost and TPE hyperparams optimization, to extract\\ninsights from different customer segments.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/hainguyen/Desktop/Harry_Nguyen_Resume.pdf'\n",
    "\n",
    "text = extract_text_from_pdf(path)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Resume Information:\n",
      "Name: Harry Nguyen\n",
      "Â§\n",
      "Email: hainguyen29031412@gmail.com\n",
      "Phone: Phone not found\n",
      "Skills: ['SQL', 'Machine Learning', 'Data Analysis', 'Python', 'AWS']\n",
      "Education: ['University of Technology Sydney', 'University of Engineering and Technology', 'Vietnam National University', 'Bachelorâ€™s Degree']\n"
     ]
    }
   ],
   "source": [
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# processed_text = model(text)\n",
    "# processed_text\n",
    "\n",
    "sample_skills = [\n",
    "        \"Python\", \"Java\", \"Machine Learning\", \"SQL\", \"JavaScript\",\n",
    "        \"Project Management\", \"Data Analysis\", \"C++\", \"React\", \"AWS\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Parse the resume\n",
    "result = parse_resume(model, path, sample_skills)\n",
    "\n",
    "# Print results\n",
    "print(\"Extracted Resume Information:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Resume Information:\n",
      "Name: HENRY LE\n",
      "Email: nhathoangle1312@gmail.com\n",
      "Phone: Phone not found\n",
      "Skills: ['SQL', 'Machine Learning', 'Data Analysis', 'Python', 'AWS']\n",
      "Education: ['UNIVERSITY OF TECHNOLOGY SYDNEY', 'KENT INSTITUTE UNIVERSITY', 'Bachelor of Accounting\\nâ€¢ Focus', 'CURTIN UNIVERSITY']\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/hainguyen/Downloads/Henry CV.pdf'\n",
    "\n",
    "result = parse_resume(model, path, sample_skills)\n",
    "\n",
    "# Print results\n",
    "print(\"Extracted Resume Information:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/linkedin-jobs-2023-2024/postings.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_id', 'company_name', 'title', 'description', 'max_salary',\n",
       "       'pay_period', 'location', 'company_id', 'views', 'med_salary',\n",
       "       'min_salary', 'formatted_work_type', 'applies', 'original_listed_time',\n",
       "       'remote_allowed', 'job_posting_url', 'application_url',\n",
       "       'application_type', 'expiry', 'closed_time',\n",
       "       'formatted_experience_level', 'skills_desc', 'listed_time',\n",
       "       'posting_domain', 'sponsored', 'work_type', 'currency',\n",
       "       'compensation_type', 'normalized_salary', 'zip_code', 'fips'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116       Request: Data ArchitectLocation: San Francisco...\n",
       "134       This opportunity is joining an innovation driv...\n",
       "165       The Enterprise Data Infrastructure and Analyti...\n",
       "283       Data Engineer with Kafka (W2 Only)ðŸ’¯% Remote\\nM...\n",
       "348       Company DescriptionPB Built is a residential c...\n",
       "                                ...                        \n",
       "123475    About This Featured Opportunity\\n\\nWe are look...\n",
       "123580    Role Title: Data Engineering Lead for a global...\n",
       "123727    Overview\\n\\nThe Credit Risk & Decision Science...\n",
       "123770    Overview\\n\\nManage Navy Federal's BSA/AML and ...\n",
       "123845    About Pinterest:\\n\\nMillions of people across ...\n",
       "Name: description, Length: 2720, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# all_titles = df['title'].unique()\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# titles = ['Data Analyst', 'Data Scientist', 'Data Engineer', 'ML Engineer', 'Machine Learning Engineer', 'AI Engineer',\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#           'AI Researcher']\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# df_filter = df[df.title.isin(titles)]\u001b[39;00m\n\u001b[32m      7\u001b[39m title_keywords = [\u001b[33m'\u001b[39m\u001b[33mData \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAI \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mML \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMachine Learning \u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df = \u001b[43mdf\u001b[49m[df.title.str.contains(\u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m.join(title_keywords), case=\u001b[38;5;28;01mFalse\u001b[39;00m, na=\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# all_titles = df['title'].unique()\n",
    "\n",
    "# titles = ['Data Analyst', 'Data Scientist', 'Data Engineer', 'ML Engineer', 'Machine Learning Engineer', 'AI Engineer',\n",
    "#           'AI Researcher']\n",
    "# df_filter = df[df.title.isin(titles)]\n",
    "\n",
    "title_keywords = ['Data ', 'AI ', 'ML ', 'Machine Learning ']\n",
    "\n",
    "df = df[df.title.str.contains('|'.join(title_keywords), case=False, na=False)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re \n",
    "import spacy\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    with pdfplumber.open(pdf_file) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def extract_name(doc: spacy.tokens.Doc) -> str:\n",
    "    \"\"\"Extract the person's name using NER (Named Entity Recognition).\"\"\"\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            return ent.text\n",
    "    # Fallback: assume name is at the top of the resume (first line)\n",
    "    first_line = doc.text.split('\\n')[0].strip()\n",
    "    return first_line if first_line else \"Name not found\"\n",
    "\n",
    "def extract_email(text: str) -> str:\n",
    "    \"\"\"Extract email address using regex.\"\"\"\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    match = re.search(email_pattern, text)\n",
    "    return match.group(0) if match else \"Email not found\"\n",
    "\n",
    "def extract_phone(text: str) -> str:\n",
    "    \"\"\"Extract phone number using regex.\"\"\"\n",
    "    phone_pattern = r'(\\+?\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
    "    match = re.search(phone_pattern, text)\n",
    "    return match.group(0) if match else \"Phone not found\"\n",
    "\n",
    "def extract_skills(doc, skills_list):\n",
    "    \"\"\"Extract skills by matching against a predefined list.\"\"\"\n",
    "    found_skills = set()\n",
    "    text = doc.text.lower()\n",
    "    for skill in skills_list:\n",
    "        if skill.lower() in text:\n",
    "            found_skills.add(skill)\n",
    "    return list(found_skills) if found_skills else [\"No skills matched\"]\n",
    "\n",
    "def extract_education(doc):\n",
    "    \"\"\"Extract education details.\"\"\"\n",
    "    education_keywords = {\"university\", \"college\", \"institute\", \"school\", \"bachelor\", \"master\", \"phd\", \"degree\"}\n",
    "    education = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"ORG\", \"DATE\"} and any(keyword in ent.text.lower() for keyword in education_keywords):\n",
    "            education.append(ent.text)\n",
    "    return education if education else [\"Education not found\"]\n",
    "\n",
    "def parse_resume(model, pdf_path, skills_list):\n",
    "    \"\"\"Main function to parse resume and extract information.\"\"\"\n",
    "    # Extract text from PDF\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if not text:\n",
    "        return {\"error\": \"No text extracted from resume\"}\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = model(text)\n",
    "\n",
    "    # Extract information\n",
    "    resume_data = {\n",
    "        \"name\": extract_name(doc),\n",
    "        \"email\": extract_email(text),\n",
    "        \"phone\": extract_phone(text),\n",
    "        \"skills\": extract_skills(doc, skills_list),\n",
    "        \"education\": extract_education(doc)\n",
    "    }\n",
    "    return resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Harry Nguyen\\n§ hainguyen2903.github.io/gitprofile | (cid:239) linkedin.com/in/nguyenphuchai | # hainguyen29031412@gmail.com\\nEDUCATION\\nUniversity of Technology Sydney (UTS) August 2023 - August 2025\\nMaster’s Degree in Data Science (Postgraduate Excellence International Scholarship)\\nUniversity of Engineering and Technology, Vietnam National University August 2018 - August 2022\\nBachelor’s Degree in Computer Science\\nGPA: 3.51/4.0 Thesis score: 9.1/10\\nSKILLS AND KNOWLEDGE\\nBackground Knowledge Machine Learning, Computer Vision, Natural Language Processing\\nData Analysis Skills Tableau, Seaborn, Matplotlib, AWS\\nProgram Languages Python, SQL\\nData Enigineer Skills Azure, Databrick, Airflow, dbt\\nSoft Skills Teamwork, Self-Studying, Leadership, Problem Solving\\nWORKING EXPERIENCE\\nMachine Learning Engineer (SiliconCube Company) July 2022 - June 2023\\n• Smart Parking System - Car Detection and License Plate Recognition\\nAn automatic parking monitoring system, which includes license plate recognition and parking slots monitoring.\\nResponsibility:\\n– Experimented, error analysed and intergrated multiple models (YOLOv5, WPOD-Net, ...), resulted in more\\nthan 98% in term of accuracy.\\n– Deployed trained model on NVIDIA Jetson Xavier device. Built server-client protocol to verified system pipeline\\nand dockerized product.\\n• Smart Advisor for Online Learning Platform\\nAn AI system for tracking student knowledge level and recommending essential learning materials based on their historical\\nacademic performance.\\nResponsibility:\\n– Experimented, error analysed and intergrated multiple deep models to achieve high performance.\\n– Created pipeline for system continuous training and monitoring using Wandb.\\nComputer Vision Research Engineer (AI Lab - UET) April 2020 - August 2022\\n• Real Time Multiple Cameras Surveillance System [Link]\\nAreal-timeMultipleTargetsTrackingsystemwith50camerasand12differentobservationareasina5-floorbuilding.\\nResponsibility:\\n– Researched, developed and ensembled several detection models for comparative analysis (mostly YOLOv5 and\\nCenterNet).\\n– Research, optimized and benchmarked different tracking models (DeepSORT, FairMOT), resulted in approxi-\\nmately 85% in term of MOTA metric on collected dataset.\\n– Leaded data team and annotation pipeline, including data collection, extraction, annotation and quality\\nverification.\\n– Created partial automation pipeline with re-identification models to support annotation process, resulted\\nin x2 annotation speed compared to traditional method.\\n– Collaborated to generate the largest multiple-camera tracking dataset compared to existing datasets in\\nterms of camera, tracking area, and human ID quantities.\\n– Collaborated to design new algorithm for tracking across multiple cameras using different techniques, namely\\ncamera calibration, top-view representation, feature matching, etc.DATA SCIENCE PROJECTS\\n• Airbnb ad-hoc analyses with Big Data November 2024\\nEnd-to-end ELT pipeline and ad hoc analysis from raw data to key insights for the optimization of Airbnb strategies.\\nResponsibility:\\n– Deployed a production-ready ELT data pipeline, automating tasks with Apache Airflow and dbt Cloud.\\n– Organized data using a medallion architecture and enabled ad-hoc analysis with data marts.\\n– Delivered ad-hoc analyses that provided actionable insights for key business decisions.\\n• NSW Traffic Analysis for Road Maintenance Optimization [Link] June 2024\\nA story-telling project with advanced visualizations for NSW Road Maintenance Optimization.\\nResponsibility:\\n– Performed different data processing and transformation techniques on the the source data set (dedupicating,\\naggregation, joining, etc.).\\n– LeveragedTableauforcomplexvisualizations,includinggeographicalheatmaps,multi-levelbarcharts,time-series\\ncharts, and combined charts.\\n– Designed storyline with effective visualizations (i.e., color highlighting, group comparisons, etc.) to deliver valu-\\nable business insights to audience.\\n• AWS Elastic Beanstalk PaaS for LAMP stack application June 2024\\nThe project aims to implement a scalable, elastic, highly available, and fault-tolerant PaaS for business needs.\\nResponsibility:\\n– Implemented a PaaS with different AWS services, including EC2, Load Balancer, Auto Scaling Group, RDS,\\nVPC, etc. that allows businesses to fully customize for their LAMP stack application.\\n• Banking Customer Segmentation [Link] June 2024\\nThe project leverages the bank’s extensive transactional data collected over the past three years to develop machine\\nlearning for segmenting customers based on spending behaviors, facilitating personalized marketing.\\nResponsibility:\\n– Performed comprehensive data exploratory to identify important features for model development.\\n– Leveragedoriginalfeaturestogenerateamorereliablefeatureset(i.e. RFMvalues)forbettercustomerclustering.\\n– ExperimentedvariousclusteringmodelswhichincludesDBSCANandKMeans,combiningwithPCAtooptimize\\nmodel weight and performance.\\n• Telecom Customer Understanding [Link] August 2023\\nThe project aims to obtain a comprehensive understanding of customer groups from company’s past marketing cam-\\npaigns to identify new strategies for the upcoming campaign through the development of the EDA and ML model\\ndevelopment.\\nResonsibility:\\n– Applied various EDA techniques (visualization, univariate and bivariate analysis, etc.) to draw comprehensive\\ninsights of customers and external contexts of the given dataset.\\n– Leveraged hypothesis testing to verify generated hypothesis of variable relationships.\\n– Developed various classification models, including XGBoost and TPE hyperparams optimization, to extract\\ninsights from different customer segments.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/hainguyen/Desktop/Harry_Nguyen_Resume.pdf'\n",
    "\n",
    "text = extract_text_from_pdf(path)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Resume Information:\n",
      "Name: Harry Nguyen\n",
      "§\n",
      "Email: hainguyen29031412@gmail.com\n",
      "Phone: Phone not found\n",
      "Skills: ['SQL', 'Machine Learning', 'Data Analysis', 'Python', 'AWS']\n",
      "Education: ['University of Technology Sydney', 'University of Engineering and Technology', 'Vietnam National University', 'Bachelor’s Degree']\n"
     ]
    }
   ],
   "source": [
    "model = spacy.load('en_core_web_sm')\n",
    "\n",
    "# processed_text = model(text)\n",
    "# processed_text\n",
    "\n",
    "sample_skills = [\n",
    "        \"Python\", \"Java\", \"Machine Learning\", \"SQL\", \"JavaScript\",\n",
    "        \"Project Management\", \"Data Analysis\", \"C++\", \"React\", \"AWS\"\n",
    "    ]\n",
    "\n",
    "\n",
    "# Parse the resume\n",
    "result = parse_resume(model, path, sample_skills)\n",
    "\n",
    "# Print results\n",
    "print(\"Extracted Resume Information:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Resume Information:\n",
      "Name: HENRY LE\n",
      "Email: nhathoangle1312@gmail.com\n",
      "Phone: Phone not found\n",
      "Skills: ['SQL', 'Machine Learning', 'Data Analysis', 'Python', 'AWS']\n",
      "Education: ['UNIVERSITY OF TECHNOLOGY SYDNEY', 'KENT INSTITUTE UNIVERSITY', 'Bachelor of Accounting\\n• Focus', 'CURTIN UNIVERSITY']\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/hainguyen/Downloads/Henry CV.pdf'\n",
    "\n",
    "result = parse_resume(model, path, sample_skills)\n",
    "\n",
    "# Print results\n",
    "print(\"Extracted Resume Information:\")\n",
    "for key, value in result.items():\n",
    "    print(f\"{key.capitalize()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/linkedin-jobs-2023-2024/postings.csv')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['job_id', 'company_name', 'title', 'description', 'max_salary',\n",
       "       'pay_period', 'location', 'company_id', 'views', 'med_salary',\n",
       "       'min_salary', 'formatted_work_type', 'applies', 'original_listed_time',\n",
       "       'remote_allowed', 'job_posting_url', 'application_url',\n",
       "       'application_type', 'expiry', 'closed_time',\n",
       "       'formatted_experience_level', 'skills_desc', 'listed_time',\n",
       "       'posting_domain', 'sponsored', 'work_type', 'currency',\n",
       "       'compensation_type', 'normalized_salary', 'zip_code', 'fips'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116       Request: Data ArchitectLocation: San Francisco...\n",
       "134       This opportunity is joining an innovation driv...\n",
       "165       The Enterprise Data Infrastructure and Analyti...\n",
       "283       Data Engineer with Kafka (W2 Only)💯% Remote\\nM...\n",
       "348       Company DescriptionPB Built is a residential c...\n",
       "                                ...                        \n",
       "123475    About This Featured Opportunity\\n\\nWe are look...\n",
       "123580    Role Title: Data Engineering Lead for a global...\n",
       "123727    Overview\\n\\nThe Credit Risk & Decision Science...\n",
       "123770    Overview\\n\\nManage Navy Federal's BSA/AML and ...\n",
       "123845    About Pinterest:\\n\\nMillions of people across ...\n",
       "Name: description, Length: 2720, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# all_titles = df['title'].unique()\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# titles = ['Data Analyst', 'Data Scientist', 'Data Engineer', 'ML Engineer', 'Machine Learning Engineer', 'AI Engineer',\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#           'AI Researcher']\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# df_filter = df[df.title.isin(titles)]\u001b[39;00m\n\u001b[32m      7\u001b[39m title_keywords = [\u001b[33m'\u001b[39m\u001b[33mData \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAI \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mML \u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mMachine Learning \u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m df = \u001b[43mdf\u001b[49m[df.title.str.contains(\u001b[33m'\u001b[39m\u001b[33m|\u001b[39m\u001b[33m'\u001b[39m.join(title_keywords), case=\u001b[38;5;28;01mFalse\u001b[39;00m, na=\u001b[38;5;28;01mFalse\u001b[39;00m)]\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df))\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# all_titles = df['title'].unique()\n",
    "\n",
    "# titles = ['Data Analyst', 'Data Scientist', 'Data Engineer', 'ML Engineer', 'Machine Learning Engineer', 'AI Engineer',\n",
    "#           'AI Researcher']\n",
    "# df_filter = df[df.title.isin(titles)]\n",
    "\n",
    "title_keywords = ['Data ', 'AI ', 'ML ', 'Machine Learning ']\n",
    "\n",
    "df = df[df.title.str.contains('|'.join(title_keywords), case=False, na=False)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
